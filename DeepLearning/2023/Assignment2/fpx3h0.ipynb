{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install roboflow\n",
    "\n",
    "# original images\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"odKT1WyVSQ7iL8vb5eeU\")\n",
    "#project = rf.workspace(\"elteikai90\").project(\"palm-trees-lfhgj\")\n",
    "#dataset = project.version(3).download(\"coco-segmentation\")\n",
    "\n",
    "\n",
    "# augmented images\n",
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"odKT1WyVSQ7iL8vb5eeU\")\n",
    "project = rf.workspace(\"elteikai90\").project(\"palm-trees-lfhgj\")\n",
    "dataset = project.version(2).download(\"coco-segmentation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Imports and Setup\n",
    "\n",
    "Instead of manually crafting diagrams I decided to use torchvision's built in functions to visualize the data. This is a good way to get a feel for the data and to make sure that the data is being loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, copy, json, requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection, wrap_dataset_for_transforms_v2\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import make_grid, draw_bounding_boxes, draw_segmentation_masks\n",
    "import torchvision.transforms.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, text\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.1: Helpers for visualizastion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 0\n",
    "left = 1\n",
    "bottom = 2\n",
    "right = 3\n",
    "\n",
    "\n",
    "def calculate_iou(bbox_a, bbox_b):\n",
    "    yA = torch.max(bbox_a[:, top], bbox_b[:, top])\n",
    "    xA = torch.max(bbox_a[:, left], bbox_b[:, left])\n",
    "\n",
    "    yB = torch.min(bbox_a[:, bottom], bbox_b[:, bottom])\n",
    "    xB = torch.min(bbox_a[:, right], bbox_b[:, right])\n",
    "\n",
    "    interArea = (xB - xA) * (yB - yA)\n",
    "\n",
    "    mask_a = (xB - xA) > 0\n",
    "    mask_b = (yB - yA) > 0\n",
    "\n",
    "    no_intersect_mask = (mask_a & mask_b) ^ 1\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (bbox_a[:, top] - bbox_a[:, bottom]) * (bbox_a[:, left] - bbox_a[:, right])\n",
    "    boxBArea = (bbox_b[:, top] - bbox_b[:, bottom]) * (bbox_b[:, left] - bbox_b[:, right])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "\n",
    "    iou = interArea.float() / (boxAArea + boxBArea - interArea).float()\n",
    "\n",
    "    iou[no_intersect_mask] = 0\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "\n",
    "def img_to_np(img):\n",
    "    img = img.cpu().numpy()\n",
    "    img = np.copy(img)\n",
    "    img = np.swapaxes(img, 0, 1)\n",
    "    img = np.swapaxes(img, 1, 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "    \n",
    "def add_bbox(ax, bbox, color, alpha=1, text=\"\"):\n",
    "    top, left, width, height = bbox\n",
    "\n",
    "    # Create a rectangle patch and add it to the axis\n",
    "    rect = patches.Rectangle((left, top), width, height, linewidth=2, edgecolor=color, facecolor='none', alpha=alpha)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    ax.text(\n",
    "        left,\n",
    "        top,\n",
    "        text,\n",
    "        fontsize=12,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"square\",\n",
    "            fc=color,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "def visualize_anchors(ax, anchors):\n",
    "    for a in anchors:\n",
    "        add_bbox(ax, torch.round(a).long(), [1.0, 1.0, 1.0], 0.2)\n",
    "        \n",
    "def visualize_rpn(ax, nms_reg, nms_cls, img, color=[1.0, 1.0, 1.0], draw_all=False):\n",
    "    np_img = img_to_np(img)\n",
    "\n",
    "    for r, c in zip(nms_reg, nms_cls):\n",
    "        if c >= 0.5 or draw_all:\n",
    "            add_bbox(ax, r, color=color, text=\"c={:.2f}\".format(c))\n",
    "\n",
    "            \n",
    "def visualize_rcnn(ax, rcnn_reg, rcnn_cls, color_map):\n",
    "    for rcnn_r, rcnn_c in zip(rcnn_reg, rcnn_cls):\n",
    "        cls, index = torch.max(rcnn_c, dim=0)\n",
    "        \n",
    "        if index == 0:\n",
    "            continue\n",
    "        \n",
    "        cls_color = color_map[0]\n",
    "        if int(index) in color_map:\n",
    "            cls_color = color_map[int(index)]\n",
    "\n",
    "        add_bbox(ax, rcnn_r, color=cls_color, text=\"class={}: {:.2f}\".format(index, cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_segmentation_to_mask(segmentation, bbox, nopad = True, width=512, height=512):\n",
    "    \"\"\"\n",
    "    Converts COCO polygon segmentation to binary mask format.\n",
    "    Assumes the image dimensions are known (width, height).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageDraw\n",
    "\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    polygon = segmentation[0]  # Assuming single polygon per object\n",
    "    img = Image.fromarray(mask)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.polygon(polygon, outline=1, fill=1)\n",
    "    mask = np.array(img)\n",
    "\n",
    "    if nopad:\n",
    "        # Crop to bbox to reduce mask size\n",
    "        x, y, w, h = map(int, bbox)\n",
    "        mask = mask[y:y+h, x:x+w]\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask_on_image(image, target, image_shape):\n",
    "    # Generate mask\n",
    "    mask = coco_segmentation_to_mask(target['segmentation'], target['bbox'], width=image_shape[1], height=image_shape[0])\n",
    "\n",
    "    # Resize mask to match the image size if needed\n",
    "    mask = np.pad(mask, ((target['bbox'][1], image_shape[0] - mask.shape[0] - target['bbox'][1]), \n",
    "                         (target['bbox'][0], image_shape[1] - mask.shape[1] - target['bbox'][0])), \n",
    "                  'constant')\n",
    "\n",
    "    # Create an RGBA version of the image with the mask applied\n",
    "    masked_image = np.concatenate([image, np.zeros((*image.shape[:2], 1), dtype=image.dtype)], axis=-1)\n",
    "    mask_color = [0, 255, 255, 128]  # Cyan, 50% transparent \n",
    "    masked_image[mask == 1] = mask_color\n",
    "\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image):\n",
    "    tensor_image = image.clone().detach()\n",
    "    tensor_image = tensor_image.type(torch.float)\n",
    "    tensor_image = tensor_image / 255.0\n",
    "    return tensor_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define the Dataset Class\n",
    "\n",
    "It needs to create a dataset class that can read images and their corresponding annotations from the COCO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PalmTreesDataset(CocoDetection):\n",
    "    def __init__(self, root, annFile, transforms=None):\n",
    "        super(PalmTreesDataset, self).__init__(root, annFile)\n",
    "        self.coco = COCO(annFile)\n",
    "        self.transforms = transforms\n",
    "\n",
    "        ids = self.coco.getCatIds()\n",
    "        le = LabelEncoder()\n",
    "        le.fit(ids)\n",
    "        self.le = le\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = super(PalmTreesDataset, self).__getitem__(idx)\n",
    "        image = item[0]\n",
    "        target = item[1]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Transformations\n",
    "\n",
    "Define the transformations for the images. I have already introduced some augmentation with Roboflow.\n",
    "> The example implemented a random flip, I use a built-in method from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    #if train:\n",
    "    #    transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToImage())\n",
    "    return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load the Data\n",
    "\n",
    "Load the data using the custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "\n",
    "# Paths to the data\n",
    "train_dir = f'{dataset.location}/train'\n",
    "train_ann = f'{dataset.location}/train/_annotations.coco.json'\n",
    "valid_dir = f'{dataset.location}/valid'\n",
    "valid_ann = f'{dataset.location}/valid/_annotations.coco.json'\n",
    "test_dir = f'{dataset.location}/test'\n",
    "test_ann = f'{dataset.location}/test/_annotations.coco.json'\n",
    "\n",
    "# Datasets\n",
    "dataset_train = PalmTreesDataset(train_dir, train_ann, get_transform(train=True))\n",
    "dataset_valid = PalmTreesDataset(valid_dir, valid_ann, get_transform(train=False))\n",
    "dataset_test = PalmTreesDataset(test_dir, test_ann, get_transform(train=False))\n",
    "\n",
    "dataset_train = wrap_dataset_for_transforms_v2(dataset_train, target_keys=(\"boxes\", \"labels\", \"masks\"))\n",
    "dataset_valid = wrap_dataset_for_transforms_v2(dataset_valid, target_keys=(\"boxes\", \"labels\", \"masks\"))\n",
    "dataset_test = wrap_dataset_for_transforms_v2(dataset_test, target_keys=(\"boxes\", \"labels\", \"masks\"))\n",
    "\n",
    "# Data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "data_loader_valid = torch.utils.data.DataLoader(\n",
    "    dataset_valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(\"\\n======================\\n\")\n",
    "print(f'train size: {len(dataset_train)}')\n",
    "print(f'valid size: {len(dataset_valid)}')\n",
    "print(f'test size: {len(dataset_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some images from the dataset\n",
    "\n",
    "I found `torchvision.utils.make_grid` clumsy, so I wrote my own function to display the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 20  # Total number of images to display\n",
    "num_columns = 5  # Number of columns in the grid\n",
    "num_rows = num_images // num_columns + (num_images % num_columns > 0)  # Calculate the number of rows needed\n",
    "\n",
    "# Create a figure with the specified number of subplots\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(num_columns * 2, num_rows * 2))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through the dataset and display each image\n",
    "for i, (image, targets) in enumerate(dataset_train):\n",
    "    if i >= num_images:\n",
    "        break  # Stop after displaying the desired number of images\n",
    "   \n",
    "    ax = axes[i]\n",
    "    ax.axis('off')  # Hide the axes\n",
    "\n",
    "    # transform the floating point image containing values in [0, 1] to [0, 255] \n",
    "    image = image.clone().detach()\n",
    "    # then transform it to uint8.    \n",
    "    image = image.type(torch.uint8)\n",
    "\n",
    "    boxes = targets[\"boxes\"]\n",
    "    colors = [\"red\"] * len(boxes)\n",
    "    annotated_image = draw_bounding_boxes(image, boxes=boxes, colors=colors, width=5)\n",
    "    \n",
    "    # display masks\n",
    "    masks = targets[\"masks\"].clone().detach()\n",
    "    masks = masks.type(torch.bool)\n",
    "    \n",
    "    masks_img = draw_segmentation_masks(annotated_image, masks, alpha=0.5)\n",
    "    ax.imshow(F.to_pil_image(masks_img))\n",
    "\n",
    "    '''\n",
    "    # display masks\n",
    "    image_np = image.permute(1, 2, 0).numpy()  # Convert to numpy and reshape for plotting\n",
    "    for target in targets:\n",
    "        masked_image = visualize_mask_on_image(image_np, target, image_np.shape)\n",
    "        masked_image = masked_image.astype(np.uint8)\n",
    "        ax.imshow(Image.fromarray(masked_image))  # Overlay the mask\n",
    "    '''\n",
    "\n",
    "# If there are any empty subplots, hide them\n",
    "for j in range(i, num_rows * num_columns):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the original COCO trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES91 = ['background', 'person', 'bike', 'auto', 'motorbike', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'hydrant' , 'sign', 'stop sign', 'parking clock', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', ' zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoes', 'glasses', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard' , 'sports ball', 'flying kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hotdog', 'pizza', 'doughnut', 'cake', 'chair ', 'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window', 'table', 'toilet', 'door', 'tv', 'laptop', 'mouse' , 'remote control', 'keyboard', 'mobile phone', 'microwave oven', 'oven', 'toaster', 'dishwasher', 'refrigerator', 'smoothie', 'book', 'clock', 'vase' , 'scissors', 'teddy bear', 'hair dryer', 'toothbrush', 'comb']\n",
    "\n",
    "model = maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "#model = torch.load('maskrcnn_weights/mask-rcnn-palmtrees.pt')\n",
    "#model.load_state_dict(torch.load('maskrcnn_weights/mask-rcnn-palmtrees_weights.pt'))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to convert image tensor to PIL\n",
    "def tensor_to_PIL(image):\n",
    "    return T.ToPILImage()(image).convert('RGB')\n",
    "\n",
    "# Get a random image and target from the data loader\n",
    "images, targets = next(iter(data_loader_train))\n",
    "image = images[random.randint(0, len(images) - 1)]\n",
    "\n",
    "# Move image to the same device as the model\n",
    "tensor_image = image_to_tensor(image)\n",
    "tensor_image = tensor_image.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model([tensor_image])\n",
    "\n",
    "# Function to visualize the image and the model predictions\n",
    "def show_prediction(image, prediction, threshold=0.5):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    pred = prediction[0]\n",
    "    # Filter out predictions with scores lower than the threshold\n",
    "    mask = pred['scores'] >= threshold\n",
    "\n",
    "    boxes = pred['boxes'][mask]\n",
    "    masks = pred['masks'][mask]\n",
    "    labels = pred['labels'][mask]\n",
    "    scores = pred['scores'][mask]\n",
    "    \n",
    "    # Convert predicted masks to binary (true/false) format\n",
    "    binary_masks = masks.squeeze(1) > 0.5  # Assuming masks shape is [num_objects, 1, H, W]\n",
    "\n",
    "    label_names = [CLASS_NAMES91[l] for l in labels.cpu().numpy()]\n",
    "\n",
    "    # Draw masks and bounding boxes\n",
    "    image = draw_segmentation_masks(image, binary_masks, alpha=0.5)\n",
    "    image = draw_bounding_boxes(image, boxes=boxes, colors='red', width=3)\n",
    "\n",
    "    # Display the label and score\n",
    "    for i, box in enumerate(boxes):\n",
    "        box = box.cpu().numpy()\n",
    "        plt.text(box[0], box[1], f'{label_names[i]}: {scores[i]:.2f}', color='white', fontsize=12, backgroundcolor='red')\n",
    "\n",
    "    plt.imshow(F.to_pil_image(image))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_prediction(image, prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build the Model\n",
    "\n",
    "Modify the build_model function to suit the number of classes in the dataset (background and palm tree).\n",
    "\n",
    "The COCO annotation format has a lot of information, but we only need the bounding box and the class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    # Load a pre-trained model\n",
    "    model = maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "    # Replace the classifier and mask predictor\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Assuming two classes (background and palm tree)\n",
    "model = build_model(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Train the Model\n",
    "\n",
    "Set up the optimizer, learning rate scheduler, and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(images, predictions, targets=None, threshold=0.5):\n",
    "    fig, axs = plt.subplots(len(images), 2, figsize=(12, len(images) * 5))\n",
    "\n",
    "    for i, (image, prediction, target) in enumerate(zip(images, predictions, targets)):\n",
    "        # Filter out predictions with scores lower than the threshold\n",
    "        mask = prediction['scores'] >= threshold\n",
    "\n",
    "        boxes = prediction['boxes'][mask]\n",
    "        masks = prediction['masks'][mask]\n",
    "        labels = prediction['labels'][mask]\n",
    "        scores = prediction['scores'][mask]\n",
    "\n",
    "        # Convert predicted masks to binary (true/false) format\n",
    "        binary_masks = masks.squeeze(1) > 0.5  # Assuming masks shape is [num_objects, 1, H, W]\n",
    "        \n",
    "        # Draw masks and bounding boxes\n",
    "        pred_image = draw_segmentation_masks(image, binary_masks, alpha=0.5)\n",
    "        pred_image = draw_bounding_boxes(pred_image, boxes=boxes, colors='red', width=3)\n",
    "        \n",
    "        \n",
    "        axs[i, 0].imshow(F.to_pil_image(pred_image))\n",
    "        axs[i, 0].set_title('Predictions')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # Display the label and score\n",
    "        for j, box in enumerate(boxes):\n",
    "            box = box.cpu().numpy()\n",
    "            axs[i, 0].text(box[0], box[1], f'{scores[j]:.2f}', color='white', fontsize=12, backgroundcolor='red')\n",
    "\n",
    "        \n",
    "        # Draw ground truth if available\n",
    "        if targets is not None:\n",
    "            boxes = target[\"boxes\"]\n",
    "            colors = [\"red\"] * len(boxes)\n",
    "            gt_image = draw_bounding_boxes(image, boxes=boxes, colors=colors, width=3)\n",
    "    \n",
    "            # display masks\n",
    "            masks = target[\"masks\"].clone().detach()\n",
    "            masks = masks.type(torch.bool)\n",
    "            gt_image = draw_segmentation_masks(gt_image, masks, alpha=0.5)\n",
    "\n",
    "            axs[i, 1].imshow(F.to_pil_image(gt_image))\n",
    "            axs[i, 1].set_title('Ground Truth')\n",
    "            axs[i, 1].axis('off')\n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "train_loss_dict_t = {}\n",
    "train_loss_dict_t['loss_classifier']=0\n",
    "train_loss_dict_t['loss_box_reg']=0\n",
    "train_loss_dict_t['loss_objectness']=0\n",
    "train_loss_dict_t['loss_rpn_box_reg']=0\n",
    "\n",
    "# SGD\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# Adam\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler (Optional): A learning rate scheduler can help adjust the learning rate during training to improve convergence.\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "lr_scheduler = None\n",
    "\n",
    "# Training Loop\n",
    "# number of epochs\n",
    "num_epochs = 10\n",
    "hist_train_classifier=[]\n",
    "hist_valid_classifier=[]\n",
    "hist_train_box_reg=[]\n",
    "hist_valid_box_reg=[]\n",
    "hist_train_objectness=[]\n",
    "hist_valid_objectness=[]\n",
    "hist_train_rpn_box_reg=[]\n",
    "hist_valid_rpn_box_reg=[]\n",
    "\n",
    "best_loss=float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "early_stop_tolerant_count=0\n",
    "early_stop_tolerant=10;\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss_dict_t = {}\n",
    "    train_loss_dict_t['loss_classifier']=0\n",
    "    train_loss_dict_t['loss_box_reg']=0\n",
    "    train_loss_dict_t['loss_objectness']=0\n",
    "    train_loss_dict_t['loss_rpn_box_reg']=0\n",
    "    train_losses_t=0\n",
    "    for images, targets in data_loader_train:\n",
    "        \n",
    "        tensor_images = [image_to_tensor(image) for image in images]\n",
    "        images = list(tensor_image.to(device) for tensor_image in tensor_images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "\n",
    "        train_loss_dict = model(images, targets)\n",
    "        \n",
    "        train_losses = sum(loss for loss in train_loss_dict.values())\n",
    "        train_loss_dict_t={x: train_loss_dict_t.get(x, 0) + train_loss_dict.get(x, 0)/len(dataset_train)*BATCH_SIZE  for x in set(train_loss_dict)}\n",
    "        train_losses_t=train_losses_t+train_losses \n",
    "        optimizer.zero_grad()\n",
    "        train_losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    \n",
    "\n",
    "    #===========================================================    \n",
    "    # Visualize some predictions\n",
    "    model.eval()\n",
    "    images, targets = next(iter(data_loader_valid))\n",
    "    tensor_images = []\n",
    "    for image in images:\n",
    "        # Move image to the same device as the model\n",
    "        tensor_image = image_to_tensor(image)\n",
    "        tensor_image = tensor_image.to(device)\n",
    "        tensor_images.append(tensor_image)\n",
    "\n",
    "    tensor_images = list(img.to(device) for img in tensor_images)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tensor_images)\n",
    "\n",
    "    visualize_predictions(images, predictions, targets)\n",
    "    model.train()\n",
    "    #===========================================================\n",
    "\n",
    "\n",
    "    # Validation    \n",
    "    with torch.no_grad():        # model.eval() would give different output\n",
    "        valid_loss_dict_t = {}\n",
    "        valid_loss_dict_t['loss_classifier']=0\n",
    "        valid_loss_dict_t['loss_box_reg']=0\n",
    "        valid_loss_dict_t['loss_objectness']=0\n",
    "        valid_loss_dict_t['loss_rpn_box_reg']=0\n",
    "        valid_losses_t=0\n",
    "        for images, targets in data_loader_valid:\n",
    "\n",
    "            tensor_images = [image_to_tensor(image) for image in images]\n",
    "            images = list(tensor_image.to(device) for tensor_image in tensor_images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            valid_loss_dict = model(images, targets)\n",
    "            \n",
    "            valid_losses = sum(loss for loss in valid_loss_dict.values())\n",
    "            valid_loss_dict_t={x: valid_loss_dict_t.get(x, 0) + valid_loss_dict.get(x, 0)/len(dataset_valid)*BATCH_SIZE  for x in set(valid_loss_dict)}\n",
    "            valid_losses_t=valid_losses_t+valid_losses  \n",
    "        valid_losses0=valid_losses/len(dataset_valid)*BATCH_SIZE \n",
    "        hist_train_classifier.append(np.array(train_loss_dict_t['loss_classifier'].cpu().detach()))\n",
    "        hist_valid_classifier.append(np.array(valid_loss_dict_t['loss_classifier'].cpu().detach()))\n",
    "        hist_train_box_reg.append(np.array(train_loss_dict_t['loss_box_reg'].cpu().detach()))\n",
    "        hist_valid_box_reg.append(np.array(valid_loss_dict_t['loss_box_reg'].cpu().detach()))\n",
    "        hist_train_objectness.append(np.array(train_loss_dict_t['loss_objectness'].cpu().detach()))\n",
    "        hist_valid_objectness.append(np.array(valid_loss_dict_t['loss_objectness'].cpu().detach()))\n",
    "        hist_train_rpn_box_reg.append(np.array(train_loss_dict_t['loss_rpn_box_reg'].cpu().detach()))\n",
    "        hist_valid_rpn_box_reg.append(np.array(valid_loss_dict_t['loss_rpn_box_reg'].cpu().detach()))\n",
    "\n",
    "        # Always save the current best model based on the validation data, and stop the training if no improvements happen after a certain epoch.    \n",
    "        early_stop_tolerant_count=early_stop_tolerant_count+1\n",
    "        if valid_losses_t < best_loss:\n",
    "            early_stop_tolerant_count=0\n",
    "            best_loss = valid_losses_t\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if early_stop_tolerant_count>=early_stop_tolerant:\n",
    "            break \n",
    "        if epoch % 1 != 0:\n",
    "            continue       \n",
    "        # Display some information along the training\n",
    "        print(\"Epoch: \", epoch, \".\") \n",
    "        print(\"Training losses: \")\n",
    "        print(\"Classifier loss: \", format(train_loss_dict_t['loss_classifier'].item(), \".2f\"))\n",
    "        print(\"Box regression loss: \", format(train_loss_dict_t['loss_box_reg'].item(), \".2f\"))\n",
    "        print(\"Objectness loss: \", format(train_loss_dict_t['loss_objectness'].item(), \".2f\"))\n",
    "        print(\"RPN box regression loss: \", format(train_loss_dict_t['loss_rpn_box_reg'].item(), \".2f\"))\n",
    "        print(\"Validation losses: \")\n",
    "        print(\"Classifier loss: \", format(valid_loss_dict_t['loss_classifier'].item(), \".2f\"))\n",
    "        print(\"Box regression loss: \", format(valid_loss_dict_t['loss_box_reg'].item(), \".2f\"))\n",
    "        print(\"Objectness loss: \", format(valid_loss_dict_t['loss_objectness'].item(), \".2f\"))\n",
    "        print(\"RPN box regression loss: \", format(valid_loss_dict_t['loss_rpn_box_reg'].item(), \".2f\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('maskrcnn_weights')==False: os.mkdir('maskrcnn_weights') \n",
    "torch.save(model, 'maskrcnn_weights/mask-rcnn-palmtrees.pt')\n",
    "torch.save(best_model_wts, 'maskrcnn_weights/mask-rcnn-palmtrees_weights.pt') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "torch.cuda.empty_cache()\n",
    "model.load_state_dict(best_model_wts)\n",
    "with torch.no_grad():        # model.eval() would give different output\n",
    "    test_loss_dict_t = {}\n",
    "    test_loss_dict_t['loss_classifier']=0\n",
    "    test_loss_dict_t['loss_box_reg']=0\n",
    "    test_loss_dict_t['loss_objectness']=0\n",
    "    test_loss_dict_t['loss_rpn_box_reg']=0\n",
    "    test_losses_t=0\n",
    "\n",
    "    for images, targets in data_loader_test:\n",
    "\n",
    "            tensor_images = [image_to_tensor(image) for image in images]\n",
    "            images = list(tensor_image.to(device) for tensor_image in tensor_images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            test_loss_dict = model(images, targets)\n",
    "\n",
    "            test_losses = sum(loss for loss in test_loss_dict.values())\n",
    "            test_loss_dict_t={x: test_loss_dict_t.get(x, 0) + test_loss_dict.get(x, 0)/len(dataset_test)*BATCH_SIZE  for x in set(test_loss_dict)}\n",
    "\n",
    "\n",
    "print(\"Test losses: \")\n",
    "print(\"Classifier loss: \", format(test_loss_dict_t['loss_classifier'].item(), \".2f\"))\n",
    "print(\"Box regression loss: \", format(test_loss_dict_t['loss_box_reg'].item(), \".2f\"))\n",
    "print(\"Objectness loss: \", format(test_loss_dict_t['loss_objectness'].item(), \".2f\"))\n",
    "print(\"RPN box regression loss: \", format(test_loss_dict_t['loss_rpn_box_reg'].item(), \".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('maskrcnn_weights/mask-rcnn-palmtrees_weights.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
