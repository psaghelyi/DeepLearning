{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEx2sEnHq1hX"
      },
      "source": [
        "# DND Exam 1\n",
        "\n",
        "## Name:\n",
        "## Neptun:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S52EB0FPTp5d"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "brrnvuYXmMp5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "klFko12BUcu5"
      },
      "outputs": [],
      "source": [
        "image = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])(Image.new('RGB', (224, 224), color=(0, 0, 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He5kEWxapEWJ"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "Your task is to implement the following architecture!\n",
        "\n",
        "![ARCHITECTURE.png](./model.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eD5SJjSjSGh1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "resnet18: torch.Size([1, 512, 7, 7])\n",
            "upper_deconv1: torch.Size([1, 256, 14, 14])\n",
            "upper_deconv1 relu: torch.Size([1, 256, 14, 14])\n",
            "upper_deconv2: torch.Size([1, 128, 28, 28])\n",
            "upper_deconv2 relu: torch.Size([1, 128, 28, 28])\n",
            "upper_deconv3: torch.Size([1, 64, 56, 56])\n",
            "upper_deconv3 relu: torch.Size([1, 64, 56, 56])\n",
            "upper_deconv4: torch.Size([1, 3, 112, 112])\n",
            "mp5: torch.Size([1, 3, 4, 4])\n",
            "lower_conv1: torch.Size([1, 256, 4, 4])\n",
            "lower_mp2: torch.Size([1, 256, 2, 2])\n",
            "lower_deconv3: torch.Size([1, 128, 8, 8])\n",
            "lower_bn4: torch.Size([1, 128, 8, 8])\n",
            "lower_mp5: torch.Size([1, 128, 4, 4])\n",
            "lower_conv6: torch.Size([1, 256, 4, 4])\n",
            "lower_concat: torch.Size([1, 512, 4, 4])\n",
            "lower_conv7: torch.Size([1, 3, 4, 4])\n",
            "concat: torch.Size([1, 6, 4, 4])\n",
            "lstm_input: torch.Size([1, 16, 6])\n",
            "lstm_output: torch.Size([1, 16, 10])\n",
            "output: torch.Size([1, 16, 10])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 10])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Custom layers for image reconstruction\n",
        "class CustomNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomNet, self).__init__()\n",
        "\n",
        "        # Load a pre-trained ResNet18 model\n",
        "        resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Remove the last two layers from ResNet18 to use it as a feature extractor\n",
        "        modules = list(resnet18.children())[:-2]\n",
        "        self.resnet18 = nn.Sequential(*modules)\n",
        "        \n",
        "        # Defining several transposed convolution layers for upsampling the features\n",
        "        self.upper_deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.upper_deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.upper_deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.upper_deconv4 = nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2)\n",
        "\n",
        "        # Max pooling layer for downscaling\n",
        "        self.upper_mp5 = nn.MaxPool2d(28)\n",
        "\n",
        "        # Additional convolution and pooling layers for further feature manipulation\n",
        "        self.lower_conv1 = nn.Conv2d(512, 256, kernel_size=1, stride=2)\n",
        "        self.lower_mp2 = nn.MaxPool2d(2, stride=2)\n",
        "        self.lower_deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=4)\n",
        "        self.lower_bn4 = nn.BatchNorm2d(128)\n",
        "        self.lower_mp5 = nn.MaxPool2d(2, stride=2)\n",
        "        self.lower_conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.lower_conv7 = nn.Conv2d(512, 3, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Activation functions\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "        # LSTM layer for sequence processing\n",
        "        self.lstm = nn.LSTM(input_size=6, hidden_size=10, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the modified ResNet18\n",
        "        resnet18 = self.resnet18(x)                             # [1, 512, 7, 7]\n",
        "        print(f'resnet18: {resnet18.shape}')\n",
        "        \n",
        "        # Forward pass through the upper branch\n",
        "        upper_deconv1 = self.upper_deconv1(resnet18)            # [1, 256, 14, 14]\n",
        "        print(f'upper_deconv1: {upper_deconv1.shape}')\n",
        "        upper_deconv1 = nn.functional.relu(upper_deconv1)       \n",
        "        print(f'upper_deconv1 relu: {upper_deconv1.shape}')\n",
        "        upper_deconv2 = self.upper_deconv2(upper_deconv1)       # [1, 128, 28, 28]\n",
        "        print(f'upper_deconv2: {upper_deconv2.shape}')\n",
        "        upper_deconv2 = nn.functional.relu(upper_deconv2)\n",
        "        print(f'upper_deconv2 relu: {upper_deconv2.shape}')\n",
        "        upper_deconv3 = self.upper_deconv3(upper_deconv2)       # [1, 64, 56, 56]\n",
        "        print(f'upper_deconv3: {upper_deconv3.shape}')\n",
        "        upper_deconv3 = nn.functional.relu(upper_deconv3)\n",
        "        print(f'upper_deconv3 relu: {upper_deconv3.shape}')\n",
        "        upper_deconv4 = self.upper_deconv4(upper_deconv3)       # [1, 3, 112, 112]\n",
        "        print(f'upper_deconv4: {upper_deconv4.shape}')\n",
        "        upper_mp5 = self.upper_mp5(upper_deconv4)               # [1, 3, 4, 4]\n",
        "        print(f'mp5: {upper_mp5.shape}')\n",
        "\n",
        "        # Forward pass through the lower branch\n",
        "        lower_conv1 = self.lower_conv1(resnet18)                # [1, 256, 4, 4]\n",
        "        print(f'lower_conv1: {lower_conv1.shape}')\n",
        "        lower_mp2 = self.lower_mp2(lower_conv1)                 # [1, 256, 2, 2]\n",
        "        print(f'lower_mp2: {lower_mp2.shape}')\n",
        "        lower_deconv3 = self.lower_deconv3(lower_mp2)           # [1, 128, 8, 8]\n",
        "        print(f'lower_deconv3: {lower_deconv3.shape}')\n",
        "        lower_bn4 = self.lower_bn4(lower_deconv3)               # [1, 128, 8, 8]\n",
        "        print(f'lower_bn4: {lower_bn4.shape}')\n",
        "        lower_mp5 = self.lower_mp5(lower_bn4)                   # [1, 128, 4, 4]\n",
        "        print(f'lower_mp5: {lower_mp5.shape}')\n",
        "        lower_conv6 = self.lower_conv6(lower_mp5)               # [1, 256, 4, 4]\n",
        "        print(f'lower_conv6: {lower_conv6.shape}')\n",
        "\n",
        "\n",
        "        # Concatenating the features\n",
        "        lower_concat = torch.cat((lower_conv1, lower_conv6), dim=1) # [1, 512, 4, 4]\n",
        "        print(f'lower_concat: {lower_concat.shape}')\n",
        "\n",
        "        lower_conv7 = self.lower_conv7(lower_concat)            # [1, 3, 4, 4]\n",
        "        print(f'lower_conv7: {lower_conv7.shape}')\n",
        "\n",
        "\n",
        "        # Concatenating the features\n",
        "        concat = torch.cat((upper_mp5, lower_conv7), dim=1)    # [1, 6, 4, 4]\n",
        "        print(f'concat: {concat.shape}')\n",
        "\n",
        "\n",
        "        # Reshaping for the LSTM input and processing through LSTM\n",
        "        lstm_input = concat.reshape(1, 16, -1)                  # [1, 16, 6]\n",
        "        print(f'lstm_input: {lstm_input.shape}')\n",
        "\n",
        "        lstm_output, _ = self.lstm(lstm_input)                  # [1, 16, 10]\n",
        "        print(f'lstm_output: {lstm_output.shape}')\n",
        "\n",
        "        # Applying sigmoid activation function\n",
        "        output = self.activation(lstm_output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomNet()\n",
        "\n",
        "# Add a batch dimension to the image if it doesn't have one\n",
        "if len(image.shape) == 3:\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Move the model and image to the same device (like CPU or GPU)\n",
        "model = model.to(device)\n",
        "image = image.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Perform inference without calculating gradients\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "\n",
        "# Print the shape of the output\n",
        "output.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7GYwt22pPvh"
      },
      "source": [
        "Expected output: [1,16,10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKldjf_tpXXe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(output.shape) == [1,16,10]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
